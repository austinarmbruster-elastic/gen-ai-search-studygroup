{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELSER the State of the Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure ELSER is deployed and started\n",
    "\n",
    "Full documentation including air gap instructions are here: https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html\n",
    "\n",
    "But the easiest way to do it is probably in Dev Tools\n",
    "\n",
    "```PUT _ml/trained_models/.elser_model_1```\n",
    "```json\n",
    "{\n",
    "  \"input\": {\n",
    "\t\"field_names\": [\"text_field\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "and then after the ELSER download from elastic's cloud repo is complete\n",
    "\n",
    "```POST _ml/trained_models/.elser_model_1/deployment/_start?deployment_id=for_search```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our State of the union speeches\n",
    "\n",
    "Let's clean up the text a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "PICKLE_FILE = \"./STATE_OF_THE_UNION.pickle\"\n",
    "\n",
    "speeches = None\n",
    "with open(PICKLE_FILE, 'rb') as f:\n",
    "    speeches = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility code\n",
    "def write_strings_to_file(strings, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for line in strings:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "def write_docs_to_file(docs, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for doc in docs:\n",
    "            file.write(doc.page_content + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| es.info()['tagline']: 'You Know, for Search'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = None\n",
    "\n",
    "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "elif 'ELASTIC_URL' in os.environ:\n",
    "  es = Elasticsearch(\n",
    "    os.environ['ELASTIC_URL'],\n",
    "    basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
    "    request_timeout=30\n",
    "  )\n",
    "else:\n",
    "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")\n",
    "\n",
    "if es:\n",
    "    ic(es.info()['tagline']) # should return cluster info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.elasticsearch import ElasticsearchStore\n",
    "\n",
    "index_name = \"elser_sotu_paragraphs\"\n",
    "\n",
    "elastic_vector_search = ElasticsearchStore(\n",
    "    es_connection=es,\n",
    "    index_name=index_name,\n",
    "    strategy=ElasticsearchStore.SparseVectorRetrievalStrategy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 215.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden-2023-02-07\n",
      "Biden-2022-03-01\n",
      "Biden-2022-03-01 has only 1 sections\n",
      "Biden-2021-04-28\n",
      "Biden-2021-04-28 has only 1 sections\n",
      "Trump-2020-02-04\n",
      "Trump-2020-02-04 has only 1 sections\n",
      "Trump-2019-02-05\n",
      "Trump-2019-02-05 has only 1 sections\n",
      "Trump-2018-01-30\n",
      "Trump-2018-01-30 has only 1 sections\n",
      "Trump-2017-02-28\n",
      "Trump-2017-02-28 has only 1 sections\n",
      "Obama-2016-01-12\n",
      "Obama-2016-01-12 has only 1 sections\n",
      "Obama-2015-01-20\n",
      "Obama-2015-01-20 has only 1 sections\n",
      "Obama-2014-01-28\n",
      "Obama-2014-01-28 has only 1 sections\n",
      "Obama-2013-02-12\n",
      "Obama-2013-02-12 has only 1 sections\n",
      "Obama-2012-01-24\n",
      "Obama-2012-01-24 has only 1 sections\n",
      "Obama-2011-01-25\n",
      "Obama-2011-01-25 has only 1 sections\n",
      "Obama-2010-01-27\n",
      "Obama-2010-01-27 has only 1 sections\n",
      "Obama-2009-02-24\n",
      "Obama-2009-02-24 has only 1 sections\n",
      "Bush43-2008-02-04\n",
      "Bush43-2007-01-29\n",
      "Bush43-2006-02-06\n",
      "Bush43-2005-02-07\n",
      "Bush43-2004-01-26\n",
      "Bush43-2003-02-03\n",
      "Bush43-2002-02-04\n",
      "Bush43-2001-03-05\n",
      "Clinton-2000-01-31\n",
      "Clinton-1999-01-25\n",
      "Clinton-1998-02-02\n",
      "Clinton-1997-02-10\n",
      "Clinton-1996-01-29\n",
      "Clinton-1995-01-30\n",
      "Clinton-1994-01-31\n",
      "Clinton-1993-02-22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 2, 'failed': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 0,\n",
    "    length_function = len,\n",
    "    is_separator_regex = True,\n",
    ")\n",
    "\n",
    "for speech in tqdm(speeches):\n",
    "    date = speech[\"date\"]\n",
    "    date_iso = speech[\"date_iso\"]\n",
    "    url = speech[\"url\"]\n",
    "    administration = speech[\"administration\"]\n",
    "    sotu_id = f\"{administration}-{date_iso}\"\n",
    "    print(sotu_id)\n",
    "\n",
    "    # if sotu_id != \"Biden-2021-04-28\": \n",
    "    #     continue\n",
    "    \n",
    "    text = speech[\"text\"].strip()\n",
    "\n",
    "    ## good for debugging\n",
    "    # write_strings_to_file([text], \"orig.txt\")\n",
    "\n",
    "    ## the speeches have page breaks that are surrounded by empty lines\n",
    "    sections = text.split(\"\\n\\n\")\n",
    "    pattern = r'^\\[\\[Page ([A-Za-z0-9]+)\\]\\]$'\n",
    "    for i, section in enumerate(sections):\n",
    "        s = section.strip()\n",
    "        if re.match(pattern, s):\n",
    "            del sections[i]\n",
    "    mergeSections = \"\\n\".join(sections)\n",
    "    \n",
    "\n",
    "    ## paragraphs of speech end with a '.' or a '!'\n",
    "    sections = re.split(r'[.!]\\n', mergeSections)\n",
    "    \n",
    "    num_sections = len(sections)\n",
    "\n",
    "    if num_sections < 5 :\n",
    "        ## some of the documents are just a wall of text\n",
    "        ## we'll parse them using a langchain chunker\n",
    "        print(f\"{sotu_id} has only {num_sections} sections\")\n",
    "\n",
    "        chunks = []\n",
    "        for sec in sections:\n",
    "            split_chunks = text_splitter.split_text(sec)\n",
    "            # print(split_chunks)\n",
    "            chunks.extend( split_chunks )\n",
    "        \n",
    "        for i, p in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=p,\n",
    "                metadata={\n",
    "                    \"chunk\": i,\n",
    "                    \"date\": date,\n",
    "                    \"date_iso\": date_iso,\n",
    "                    \"url\": url,\n",
    "                    \"administration\": administration,\n",
    "                    \"sotu_id\": sotu_id\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        # merged = \"\\n\".join(chunks)\n",
    "        # write_strings_to_file([merged], f\"{sotu_id}-scratch.txt\")\n",
    "\n",
    "    else:\n",
    "        # get rid of the line breaks that are just formatting\n",
    "        paragraphs = []\n",
    "        for sec in sections:\n",
    "            paragraphs.append( sec.replace(\"\\n\",\" \").strip()  )\n",
    "\n",
    "        ## We now have reasonably clean paragraphs\n",
    "\n",
    "        ## create documents\n",
    "        \n",
    "        for i, p in enumerate(paragraphs):\n",
    "            doc = Document(\n",
    "                page_content=p,\n",
    "                metadata={\n",
    "                    \"chunk\": i,\n",
    "                    \"date\": date,\n",
    "                    \"date_iso\": date_iso,\n",
    "                    \"url\": url,\n",
    "                    \"administration\": administration,\n",
    "                    \"sotu_id\": sotu_id\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        # good for debugging\n",
    "        # print(url)\n",
    "        # write_strings_to_file(paragraphs, f\"{sotu_id}-scratch.txt\")\n",
    "\n",
    "results = elastic_vector_search.add_documents(\n",
    "    documents,\n",
    "    bulk_kwargs={\n",
    "        \"chunk_size\": 16,\n",
    "        \"max_chunk_bytes\": 200000000\n",
    "    }\n",
    ")\n",
    "elastic_vector_search.client.indices.refresh(index=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
