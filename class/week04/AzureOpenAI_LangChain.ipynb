{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure hosted OpenAI and LangChain\n",
    "\n",
    "your .env file will need these\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_API_KEY=\"YOUR KEY\"\n",
    "AZURE_OPENAI_API_INSTANCE_NAME=\"https://YOUR_AZURE_KEY_ENDPOINT.com\"\n",
    "AZURE_OPENAI_API_DEPLOYMENT_NAME=\"THE_DEPLOYMENT_NAME\"\n",
    "AZURE_OPENAI_API_TYPE=\"azure\"\n",
    "AZURE_OPENAI_API_VERSION=\"2023-05-15\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openai==0.27.8 langchain==0.0.227 python_dotenv\n",
    "import os\n",
    "import openai\n",
    "import langchain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "openai.api_key = os.environ['AZURE_OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['AZURE_OPENAI_API_INSTANCE_NAME']\n",
    "openai.default_model = os.environ['AZURE_OPENAI_API_DEPLOYMENT_NAME']\n",
    "openai.api_version = os.environ['AZURE_OPENAI_API_VERSION']\n",
    "openai.api_type = os.environ['AZURE_OPENAI_API_TYPE']\n",
    "\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# llm = ChatOpenAI(\n",
    "#     temperature=0.3,\n",
    "#     model=default_model\n",
    "# )\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "\n",
    "default_model = openai.default_model\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=default_model,\n",
    "    model_name=default_model,\n",
    "    openai_api_base=openai.api_base,\n",
    "    openai_api_key=openai.api_key,\n",
    "    openai_api_type=openai.api_type, \n",
    "    openai_api_version=openai.api_version\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boris Johnson.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's do a simple completion\n",
    "llm.predict(\"The prime minister of the United Kingdom is \",\n",
    "    engine=default_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That’s right.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You could do a more complex completion and it would handle it with no issues\n",
    "chat= \"\"\"\n",
    "Lou Costello: All I’m trying to find out is what’s the guy’s name on first base.\n",
    "\n",
    "Bud Abbott: No. What is on second base.\n",
    "\n",
    "Lou Costello: I’m not asking you who’s on second.\n",
    "\n",
    "Bud Abbott: Who’s on first.\n",
    "\n",
    "Lou Costello: One base at a time!\n",
    "\n",
    "Bud Abbott: Well, don’t change the players around.\n",
    "\n",
    "Lou Costello: I’m not changing nobody!\n",
    "\n",
    "Bud Abbott: Take it easy, buddy.\n",
    "\n",
    "Lou Costello: I’m only asking you, who’s the guy on first base?\n",
    "\n",
    "Bud Abbott: That’s right.\n",
    "\n",
    "Lou Costello: Ok.\n",
    "\n",
    "Bud Abbott: All right.\n",
    "\n",
    "Lou Costello: What’s the guy’s name on first base?\n",
    "\n",
    "Bud Abbott:\n",
    "\"\"\"\n",
    "\n",
    "### The large language model is not always funny\n",
    "llm.predict(chat,\n",
    "    engine = default_model,\n",
    "    temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chat bot With memory in LangChain with OpenAI\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "import json\n",
    "# pretty printing JSON objects\n",
    "def json_pretty(input_object):\n",
    "  print(json.dumps(input_object, indent=4))\n",
    "\n",
    "\n",
    "import textwrap\n",
    "# wrap text when printing, because colab scrolls output to the right too much\n",
    "def wrap_text(text, width):\n",
    "    wrapped_text = textwrap.wrap(text, width)\n",
    "    return '\\n'.join(wrapped_text)\n",
    "\n",
    "template = \"\"\"The following is a serious conversation between a human and a TV\n",
    "News Anchor named Newsy McNewserson.\n",
    "The Anchor provides autoritative information and commentary in short responses.\n",
    "If the Anchor does not know the answer to a question,\n",
    "it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Anchor:\"\"\"\n",
    "\n",
    "MEMORY = ConversationBufferWindowMemory(ai_prefix=\"Anchor\", k=2)\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = LLMChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=MEMORY\n",
    ")\n",
    "\n",
    "def chatLoop():\n",
    "  print(\" -- Have a conversation with a TV news Anchor: \")\n",
    "  print(\" -- Ask this AI \\\"what is in the news?\\\" \")\n",
    "  print(\" -- type 'exit' when done\")\n",
    "\n",
    "  user_input = input(\"> \")\n",
    "  while not user_input.lower().startswith(\"exit\"):\n",
    "      print( conversation.run(input=user_input) )\n",
    "      print(\" -- type 'exit' when done\")\n",
    "      user_input = input(\"> \")\n",
    "  print(\"\\n -- end conversation --\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Have a conversation with a TV news Anchor: \n",
      " -- Ask this AI \"what is in the news?\" \n",
      " -- type 'exit' when done\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a serious conversation between a human and a TV\n",
      "News Anchor named Newsy McNewserson.\n",
      "The Anchor provides autoritative information and commentary in short responses.\n",
      "If the Anchor does not know the answer to a question,\n",
      "it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello\n",
      "Anchor:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello there. How can I assist you today?\n",
      " -- type 'exit' when done\n",
      "\n",
      " -- end conversation --\n"
     ]
    }
   ],
   "source": [
    "## start a new chat each time\n",
    "MEMORY.clear()\n",
    "## start the chat\n",
    "chatLoop()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
