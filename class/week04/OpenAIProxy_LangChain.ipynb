{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain OpeanAI Proxy\n",
    "\n",
    "You may have access to a large langugage model through a HTTP Proxy like this one\n",
    "https://github.com/derickson/ExpressOpenAIChatProxy\n",
    "\n",
    "Assuming you know the URL for where it is hosted and the KEY for the proxy here is the code for using it.\n",
    "\n",
    "your .env file will need this\n",
    "\n",
    "```\n",
    "PROXY_OPENAI_API_KEY=\"YOUR KEY\"\n",
    "PROXY_OPENAI_API_BASE=\"HTTPS://YOUR URL ENDING IN /v1\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openai langchain python_dotenv\n",
    "import os\n",
    "import openai\n",
    "import langchain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI libraries like for either the environment variable or the openai.api_key to be set\n",
    "load_dotenv(\".env\", override=True)\n",
    "openai.api_key = os.environ[\"PROXY_OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"PROXY_OPENAI_API_BASE\"]\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "default_model = \"gpt-3.5-turbo\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_key=openai.api_key,\n",
    "  openai_api_base=openai.api_base,\n",
    "  temperature=0.3,\n",
    "  model=default_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boris Johnson.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's do a simple completion\n",
    "llm.predict(\"The Prime Minister of the United Kingdom is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The color of the sky is purple today.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_context = \"The color of the sky is purple today\"\n",
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a helpful AI assistant that answers questions using the provided context.\n",
    "If you can't answer the question using the provided context, say \"I don't know\".\n",
    "Don't apologize, and don't say you are an AI assistant.\n",
    "\n",
    "Context: {retrieved_context}\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is the sky like today?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\", \"content\": system_prompt},\n",
    "    {\"role\":\"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "                        model=default_model,\n",
    "                        max_tokens=100,\n",
    "                        temperature=0,\n",
    "                        messages=messages\n",
    "                      )\n",
    "\n",
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
